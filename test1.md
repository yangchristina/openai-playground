That's quite a list of topics. Here's a brief overview:

**1. Finite State Text Processing & Morphology**: Finite state tools are used to analyze and generate text sequences depending on state-to-state transitions. Morphology, the study of words and how they are formed, often uses finite state automata.

**2. Pynini**: It's a Python library used to manipulate finite state automata/transducers, often used in tasks like text normalisation or spelling correction.

**3. Text normalization and Spelling**: Text normalization is the process of transforming text into a consistent form, like turning "u" to "you". Spelling correction is an error correction task that aims to correct spelling mistakes in a text.

**4. Finite State Automata (FSA), Regular Expressions, Conditional Probabilities, Bayes**: FSA is a computation model that can be in exactly one of a finite number of states at any given time. Regular expressions are sequences of characters defining search patterns. Both are used in text processing tasks. Conditional Probability and Bayes theorem are probability concepts; they're often used in spam filters, document classification etc.

**5. Language Models: Traditional vs Neural**: Language models predict the likelihood of a sequence of words. Traditional models like n-grams or Markov models ignore context and semantics. Neural language models (like RNN, LSTM, Transformer) leverage deep learning to overcome these shortcomings.

**6. Text Classification - Traditional Methods (Naive Bayes and Logistic Regression)**: These are machine learning algorithms used for categorizing text into predefined classes.

**7. Feedforward Neural Networks (MLP)**: Multi-layer Perceptrons are a type of artificial neural network that's often used for classification tasks.

**8. Text Classification - Neural Methods (MLP and CNN)**: These methods leverage neural networks to classify text, often outperforming traditional methods.

**9. Sequence labeling: Markov Models, POS tagging, NER**: These tasks involve labelling individual components in a sequence of data; like identifying nouns or named entities in a sentence.

**10. Sequence-to-Sequence: Encoder-Decoder, Attention**: Seq2Seq models are used in tasks like translation, where each input can have different lengths. They use an encoder to compress input and a decoder to recreate output. Attention mechanism helps them to focus on specific parts of the input sequence.

**11. Transformers**: A game-changing architecture in NLP, it uses self-attention mechanisms and has shown incredible results in tasks like translation, summarization etc.

**12. Pre-trained language models**: Models like BERT, GPT-2 that come pre-trained on large corpora. They save time & resources and improve performance, especially if you have less training data.

**13. Syntax, Context Free Grammars, Parsing, Chunking, Dependency Parsing, Treebanks**: These terms are related to syntactic analysis of sentences. It involves determining the structure of sentences, grouping words (chunking), and understanding the relationships between words (dependency parsing). Treebanks are annotated data used for these tasks.

**14. Semantics & Semantic Role Labeling**: Semantics involves understanding the meaning of words and sentences. Semantic role labeling is the task of assigning roles like 'agent', 'patient' to constituents of a sentence.

**15. Topic Modeling, word embeddings, Lexical Semantics**: Topic modeling is the task of identifying topics in a set of documents. Word embeddings (like Word2Vec or GloVe) are vector representations of words capturing their semantic properties. Lexical semantics involves the study of word meanings and relationships.

**16. Discourse and Coreference, Summarization**: Discourse is about understanding the connected series of sentences, and coreference occurs when multiple expressions in a sentence or document refer to the same person or thing. Summarization is the process of shortening a text while preserving its main points.

Finally, these theories/techniques can be applied to accomplish tasks such as text translation, sentiment analysis, name entity recognition, speech recognition, document summarization, spam detection, question answering, and information extraction among others.

It's a vast field! And we've only scratched the surface in these descriptions. Let me know if there's a specific area you want to deep dive into!